{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Colab----#\n",
    "import os\n",
    "!git clone https://github.com/ARIS2333/My_Project.git\n",
    "os.chdir('./My_Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----system imports----#\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "#----sklearn imports----#\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#----pytorch imports----#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#----custom imports----#\n",
    "from processing_utils import downloader, mat_extractor, cropper, save_processed_data\n",
    "from train_utils import *\n",
    "from eva_utils import show_acc_loss_test, evaluation_test, save_results, summary_results\n",
    "\n",
    "#----settings----#\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    path  = 'data/raw'\n",
    "    dpath = downloader(path, subjects=list(range(1,10)), \n",
    "                       url=\"http://bnci-horizon-2020.eu/database/data-sets/001-2014/\")\n",
    "    return dpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data():\n",
    "    tr_names = ['A0' + str(i) + 'T.mat' for i in range(1,10)]\n",
    "    te_names = ['A0' + str(i) + 'E.mat' for i in range(1,10)]\n",
    "\n",
    "    dpath_train_all = []\n",
    "    dpath_test_all = []\n",
    "\n",
    "    for id in range(len(tr_names)):\n",
    "        print(f'*** Subject {id+1} ***\\n')\n",
    "\n",
    "        # Extracting the data from the mat files, and applying the preprocessing steps\n",
    "        x_train, y_train = mat_extractor(path=dpath/tr_names[id], channel_norm = config_processor['if_normalize'], remove_eog = config_processor['if_remove_eog'],\n",
    "                                        bpf_dict={'apply':config_processor['if_filter'], 'fs': 250, 'lc':config_processor['lc'], 'hc':config_processor['hc'], 'order':config_processor['order']})\n",
    "        x_test , y_test  = mat_extractor(path=dpath/te_names[id], channel_norm = config_processor['if_normalize'], remove_eog = config_processor['if_remove_eog'],\n",
    "                                        bpf_dict={'apply':config_processor['if_filter'], 'fs': 250, 'lc':config_processor['lc'], 'hc':config_processor['hc'], 'order':config_processor['order']})\n",
    "       \n",
    "       # Changing the labels to start from 0\n",
    "        y_train, y_test = y_train-1, y_test-1\n",
    "\n",
    "        # Cropping the data to increase the number of samples\n",
    "        if config_processor['if_cropper']:\n",
    "            x_train, y_train = cropper(x_train, y_train, window=config_processor['cropper_window'], step=config_processor['cropper_step'])\n",
    "            x_test , y_test  = cropper(x_test , y_test, window=config_processor['cropper_window'], step=config_processor['cropper_step'])\n",
    "            \n",
    "        # save the data to another folder\n",
    "        dpath_train = save_processed_data(config_processor['save_path_train'], x_train, y_train, 'A0' + str(id+1)+'T.pt')\n",
    "        dpath_test = save_processed_data(config_processor['save_path_test'], x_test, y_test, 'A0' + str(id+1)+'E.pt')\n",
    "        dpath_train_all.append(dpath_train)\n",
    "        dpath_test_all.append(dpath_test)\n",
    "\n",
    "    return dpath_train_all, dpath_test_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Download data----#\n",
    "dpath = download_data()\n",
    "\n",
    "#----configuration----#\n",
    "config_processor = {\n",
    "    #config for cropper\n",
    "    'if_cropper': False,\n",
    "    'cropper_window': 500,\n",
    "    'cropper_step': 500,\n",
    "\n",
    "    #config for filter\n",
    "    'if_filter': False,\n",
    "    'lc': 4,\n",
    "    'hc': 40,\n",
    "    'order': 3,\n",
    "\n",
    "    # config for normalizer\n",
    "    'if_normalize': False,\n",
    "\n",
    "    # config for EOG\n",
    "    'if_remove_eog': False,\n",
    "\n",
    "    # config for saving\n",
    "    'save_path_train': 'data/processed/train',\n",
    "    'save_path_test': 'data/processed/test',\n",
    "    }\n",
    "\n",
    "#----process data----#\n",
    "dpath_train_all, dpath_test_all = processing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Torch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic=True\n",
    "    # torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_dataset(Dataset):\n",
    "    def __init__(self,file_path,transform=None):\n",
    "        self.file_path = file_path\n",
    "        self.data, self.label = self.parse_data_file(file_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def parse_data_file(self,file_path):\n",
    "        pt_file = torch.load(file_path)\n",
    "        data, label = pt_file['data'], pt_file['label']\n",
    "\n",
    "        # data\n",
    "        data = torch.tensor(data)\n",
    "        data = data.unsqueeze(1)\n",
    "        # label\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        return data.float(), label.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        target = self.label[index]\n",
    "\n",
    "        return x, target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoader(train_path, test_path, \n",
    "                  data_transforms = transforms.Compose([transforms.ToTensor()]), \n",
    "                  batch_size=32):\n",
    "    set_seed(config_train['seed'])\n",
    "\n",
    "    train_dataset = feature_dataset(train_path, transform=data_transforms)\n",
    "    test_dataset = feature_dataset(test_path, transform=data_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Model blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a block that enables us to enter customized functions in the structure of an nn.Module\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "# a function for adding Flatten layers to Conv2d architectures\n",
    "def myreshape(xb):\n",
    "    return xb.view(-1,xb.shape[1]*xb.shape[3])\n",
    "\n",
    "\n",
    "# constrained blocks are required for implementing of EEGNet and ShallowConvNet\n",
    "class Conv2dConstrained(nn.Conv2d):\n",
    "    def __init__(self, *args, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dConstrained, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p = 2, dim = 0, maxnorm = self.max_norm\n",
    "        )\n",
    "        return super(Conv2dConstrained, self).forward(x)\n",
    "\n",
    "class LinearConstrained(nn.Linear):\n",
    "    def __init__(self, *args, max_norm = 0.25, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(LinearConstrained, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p = 2, dim = 0, maxnorm = self.max_norm\n",
    "        )\n",
    "        return super(LinearConstrained, self).forward(x)\n",
    "    \n",
    "# a class that allows us to define linear layers without specifying in_features\n",
    "class LinearModified(nn.Module):\n",
    "    def __init__(self, out_features, bias=False, max_norm=None):\n",
    "        super().__init__()\n",
    "        self.in_features = None\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.max_norm = max_norm\n",
    "        self.__built = False\n",
    "        self.lin = 0\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        assert xb.ndim == 2, 'xb should have 2 dimensions'\n",
    "        if self.__built == False:\n",
    "            self.__built = True\n",
    "            self.in_features = xb.shape[1]\n",
    "            dev = 'cpu' if xb.get_device == -1 else 'cuda'\n",
    "            if self.max_norm == None:\n",
    "                self.lin = nn.Linear(self.in_features, self.out_features, bias=self.bias).to(dev)\n",
    "            else:\n",
    "                self.lin = LinearConstrained(self.in_features, self.out_features, max_norm=self.max_norm, bias=self.bias).to(dev)\n",
    "        xb = self.lin(xb)\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, classes_num):\n",
    "        super(mine, self).__init__()\n",
    "\n",
    "        self.drop_out = 0.5\n",
    "\n",
    "        self.block_1 = nn.Sequential(\n",
    "            # Pads the input tensor boundaries with zero\n",
    "            # left, right, up, bottom\n",
    "            nn.ZeroPad2d((31+62, 32+63, 0, 0)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,  # input shape (1, C, T)\n",
    "                out_channels=8,  # num_filters\n",
    "                kernel_size=(1, 64),  # filter size\n",
    "                bias=False\n",
    "            ),  # output shape (8, C, T)\n",
    "            nn.BatchNorm2d(8)  # output shape (8, C, T)\n",
    "        )\n",
    "\n",
    "        # block 2 and 3 are implementations of Depthwise Convolution and Separable Convolution\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,  # input shape (8, C, T)\n",
    "                out_channels=16,  # num_filters\n",
    "                kernel_size=(22, 1),  # filter size\n",
    "                groups=8,\n",
    "                bias=False\n",
    "            ),  # output shape (16, 1, T)\n",
    "            nn.BatchNorm2d(16),  # output shape (16, 1, T)\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)),  # output shape (16, 1, T//4)\n",
    "            nn.Dropout(self.drop_out)  # output shape (16, 1, T//4)\n",
    "        )\n",
    "\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,  # input shape (16, 1, T//4)\n",
    "                out_channels=16,  # num_filters\n",
    "                kernel_size=(1, 16),  # filter size\n",
    "                groups=16,\n",
    "                bias=False\n",
    "            ),  # output shape (16, 1, T//4)\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,  # input shape (16, 1, T//4)\n",
    "                out_channels=16,  # num_filters\n",
    "                kernel_size=(1, 1),  # filter size\n",
    "                bias=False\n",
    "            ),  # output shape (16, 1, T//4)\n",
    "            nn.BatchNorm2d(16),  # output shape (16, 1, T//4)\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),  # output shape (16, 1, T//32)\n",
    "            nn.Dropout(self.drop_out)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear((16 * 35), classes_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        # print(\"block1\", x.shape)\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        # print(\"block2\", x.shape)\n",
    "\n",
    "        x = self.block_3(x)\n",
    "        # print(\"block3\", x.shape)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.out(x)\n",
    "        # return F.softmax(x, dim=1), x  # return x for visualization\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                            Configurations                                  #\n",
    "##############################################################################\n",
    "config_train = {\n",
    "    # Name\n",
    "    'trial': '1', # trial number, used for naming the saved files\n",
    "\n",
    "    # Hyperparameters\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 5000,\n",
    "    'weight_decay': 1e-4,\n",
    "    'seed': 42,\n",
    "    'device': torch.device('cuda'),\n",
    "\n",
    "    # Early stopping\n",
    "    'if_EarlyStop': True, # if we want to use early stopping\n",
    "    'patience': 300, # number of epochs to wait before early stopping\n",
    "    'attribute': 'acc', # attribute to monitor for early stopping, 'acc' or 'loss'\n",
    "\n",
    "    # Print\n",
    "    'period': 100 # print every period epochs\n",
    "}\n",
    "set_seed(config_train['seed'])\n",
    "model_init = EEGNet(classes_num=4).to(config_train['device'])\n",
    "optimizer_init = optim.Adam(model_init.parameters(),lr=config_train['lr'],weight_decay=config_train['weight_decay'], eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "##############################################################################\n",
    "#                            Training                                        #\n",
    "##############################################################################\n",
    "#----Lists for each subject----#\n",
    "history_allSubjects = []\n",
    "checkpoint_allSubjects = []\n",
    "\n",
    "for subject in range(9):\n",
    "    print('#########################################\\n#########################################\\n#########################################')\n",
    "    print(f'----------------Subject {subject+1}----------------\\n')\n",
    "\n",
    "    #----DataLoader----#\n",
    "    train_dataloader, test_dataloader = getDataLoader(dpath_train_all[subject], dpath_test_all[subject], batch_size=config_train['batch_size'])\n",
    "    #----Model, Optimizer, Loss----#\n",
    "    model = model_init\n",
    "    optimizer = optimizer_init\n",
    "    loss_fn = criterion.to(config_train['device'])\n",
    "    #----Training----#\n",
    "    e_stop = EarlyStopping(state=config_train['if_EarlyStop'], patience=config_train['patience'], attribute=config_train['attribute'])\n",
    "    results = train(model, optimizer,\n",
    "                    train_dataloader, test_dataloader,\n",
    "                    epochs=config_train['epochs'],\n",
    "                    loss_func=loss_fn, period=config_train['period'],\n",
    "                    er_stop=e_stop)\n",
    "\n",
    "\n",
    "    history, checkpoint = results\n",
    "   \n",
    "        \n",
    "    history_allSubjects.append(history)\n",
    "    checkpoint_allSubjects.append(checkpoint)\n",
    "\n",
    "##############################################################################\n",
    "#                            Evaluation                                      #\n",
    "##############################################################################\n",
    "# acc_all, kappa_all, precision_all, recall_all = [], [], [], []\n",
    "# for i in range(9):\n",
    "#     print(f'*** Subject {i+1} ***\\n')\n",
    "\n",
    "#     checkpoint = checkpoint_allSubjects[i]\n",
    "#     history = history_allSubjects[i]\n",
    "    \n",
    "#     # checkpoint = checkpoint_allSubjects[0]\n",
    "#     # history = history_allSubjects[0]\n",
    "#     model = model_init\n",
    "#     _, test_dataloader = getDataLoader(dpath_train_all[i], dpath_test_all[i], batch_size=config_train['batch_size'])\n",
    "#     show_acc_loss_test(history, subject=i)\n",
    "#     acc, kappa, precision, recall = evaluation_test(config_train, checkpoint, model, test_dataloader, subject=i) if checkpoint is not None else (None, None, None, None)\n",
    "    \n",
    "#     acc_all.append(acc)\n",
    "#     kappa_all.append(kappa)\n",
    "#     precision_all.append(precision)\n",
    "#     recall_all.append(recall)\n",
    "    \n",
    "# print('\\n\\n')\n",
    "# print('*'*50)\n",
    "# print('*** Summary ***')\n",
    "# print(f'Accuracy: {np.mean(acc_all):.2f} +/- {np.std(acc_all):.2f}')\n",
    "# print(f'Kappa: {np.mean(kappa_all):.2f} +/- {np.std(kappa_all):.2f}')\n",
    "# print(f'Precision: {np.mean(precision_all):.2f} +/- {np.std(precision_all):.2f}')\n",
    "# print(f'Recall: {np.mean(recall_all):.2f} +/- {np.std(recall_all):.2f}')\n",
    "\n",
    "##############################################################################\n",
    "#                            Saving Results                                  #\n",
    "##############################################################################\n",
    "acc_all, kappa_all, precision_all, recall_all = [], [], [], []\n",
    "for i in range(9):\n",
    "    checkpoint = checkpoint_allSubjects[i]\n",
    "    history = history_allSubjects[i]\n",
    "    model = model_init\n",
    "    _, test_dataloader = getDataLoader(dpath_train_all[i], dpath_test_all[i], batch_size=config_train['batch_size'])\n",
    "    savings = save_results(save_dir='results', \n",
    "                        trial=config_train['trial'],\n",
    "                        subject=str(i+1))\n",
    "    savings.info_Preprocessing(config_processor)\n",
    "    savings.info_Model(config_train)\n",
    "    savings.info_Training(history)\n",
    "    savings.info_model_optimizer(checkpoint)\n",
    "    savings.show_acc_loss(history)\n",
    "    acc, kappa, precision, recall = savings.evaluation(config_train, checkpoint, model, test_dataloader)\n",
    "    savings.write_TensorBoard(history)\n",
    "\n",
    "    acc_all.append(acc)\n",
    "    kappa_all.append(kappa)\n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)\n",
    "\n",
    "summary_results(acc_all, kappa_all, precision_all, recall_all, save_dir='results', trial=config_train['trial'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
